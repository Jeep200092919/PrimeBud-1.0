import streamlit as st
import requests
import json
import concurrent.futures

# ==============================
# CONFIGURA√á√ïES GERAIS
# ==============================
st.set_page_config(page_title="PrimeBud Turbo 1.0", page_icon="ü§ñ", layout="wide")

OLLAMA_URL = "http://localhost:11434/api/chat"
GITHUB_URL = "https://github.com/Jeep200092919/PrimeBud-Turbo-1.0"

# ==============================
# MODELOS (LLaMA 3 no lugar do Mistral)
# ==============================
MODEL_IDS = {
    "LLaMA 3": "llama3",
    "CodeGemma 7B": "codegemma:7b",
    "Phi-3": "phi3",
}

# ==============================
# BASE DE USU√ÅRIOS (come√ßa simples)
# ==============================
if "usuarios" not in st.session_state:
    st.session_state.usuarios = {"teste": {"senha": "0000", "plano": "Free"}}

# ==============================
# DESCRI√á√ïES DE MODO
# ==============================
MODOS_DESC = {
    "‚ö° Flash": "Respostas curtas e instant√¢neas (LLaMA 3).",
    "üîµ Normal": "Respostas equilibradas e naturais (LLaMA 3, turbo).",
    "üçÉ Econ√¥mico": "Respostas curtas e otimizadas (LLaMA 3).",
    "üí¨ Mini": "Conversas leves e sem c√≥digo (LLaMA 3).",
    "üíé Pro (Beta)": "C√≥digo + explica√ß√£o t√©cnica curta (CodeGemma 7B).",
    "‚òÑÔ∏è Ultra (Beta)": "Pipeline turbo (LLaMA 3 ‚Üí CodeGemma 7B ‚Üí Phi-3).",
    "‚úçÔ∏è Escritor": "Textos criativos (5‚Äì10 linhas) muito r√°pidos (Phi-3).",
    "üè´ Escola": "Ajuda escolar (1¬∫‚Äì3¬∫ EM) ‚Äî dupla r√°pida (Phi-3 + LLaMA 3).",
    # Modos profissionais (Assinatura Trabalho / Ultra / Professor)
    "üë®‚Äçüè´ Professor": "Explica conte√∫dos, cria plano de aula e exerc√≠cios (pipeline LLaMA 3 ‚Üí CodeGemma ‚Üí Phi-3).",
    "üé® Designer": "Ideias visuais, UI/UX, prompts de imagem e estrutura de layout (pipeline LLaMA 3 ‚Üí CodeGemma ‚Üí Phi-3).",
    "üíª Codificador": "Gera, corrige e explica c√≥digo limpo (pipeline LLaMA 3 ‚Üí CodeGemma ‚Üí Phi-3).",
    "üß© Estrat√©gias": "Estrat√©gias de neg√≥cio/marketing/tech claras e acion√°veis (pipeline LLaMA 3 ‚Üí CodeGemma ‚Üí Phi-3).",
}

# ==============================
# LIMITES DE MODELOS POR MODO
# ==============================
MODE_LIMITS = {
    "‚ö° Flash": ["LLaMA 3"],
    "üîµ Normal": ["LLaMA 3"],
    "üçÉ Econ√¥mico": ["LLaMA 3"],
    "üí¨ Mini": ["LLaMA 3"],
    "üíé Pro (Beta)": ["CodeGemma 7B"],
    "‚òÑÔ∏è Ultra (Beta)": ["PIPELINE"],  # LLaMA3 -> CodeGemma -> Phi-3
    "‚úçÔ∏è Escritor": ["Phi-3"],
    "üè´ Escola": ["DUO"],             # Phi-3 + LLaMA 3
    # Modos Trabalho / Professor usam pipeline WORK dedicada
    "üë®‚Äçüè´ Professor": ["WORK"],
    "üé® Designer": ["WORK"],
    "üíª Codificador": ["WORK"],
    "üß© Estrat√©gias": ["WORK"],
}

# ==============================
# PROMPTS DE ESTILO (objetivos = maior velocidade)
# ==============================
MODE_PROMPTS = {
    "‚ö° Flash": "Responda em at√© 2 frases, direto e sem rodeios.",
    "üîµ Normal": "Explique de forma clara e breve (at√© 6 linhas).",
    "üçÉ Econ√¥mico": "M√°x. 3 linhas, direto ao ponto.",
    "üí¨ Mini": "Converse de modo leve e simples. N√£o gere c√≥digo.",
    "üíé Pro (Beta)": "Forne√ßa o c√≥digo e breve explica√ß√£o t√©cnica (‚â§5 linhas).",
    "‚úçÔ∏è Escritor": "Crie um texto coeso e criativo de 5‚Äì10 linhas.",
    "üè´ Escola": "Explique e resolva d√∫vidas do Ensino M√©dio de forma did√°tica e r√°pida.",
}

# ==============================
# FUN√á√ÉO B√ÅSICA DE CHAMADA AO OLLAMA (com timeout e stream=False)
# ==============================
def chat_ollama(model: str, prompt: str, options: dict | None = None, timeout: int = 30) -> str:
    payload = {
        "model": model,
        "stream": False,
        "messages": [
            {"role": "system", "content": "Voc√™ √© o PrimeBud. Seja objetivo, claro e √∫til."},
            {"role": "user", "content": prompt}
        ]
    }
    if options:
        payload["options"] = options

    r = requests.post(OLLAMA_URL, json=payload, timeout=timeout)
    try:
        data = r.json()
    except json.JSONDecodeError:
        # fallback caso venham m√∫ltiplas linhas JSON no buffer
        data = json.loads(r.text.strip().split("\n")[0])
    return data["message"]["content"]

# ==============================
# PIPELINES OTIMIZADAS (Turbo)
# ==============================

# Ultra: LLaMA 3 (resumo/entendimento) -> CodeGemma (c√≥digo) -> Phi-3 (refino/explica√ß√£o)
def gerar_resposta_ultra(msg: str) -> str:
    with concurrent.futures.ThreadPoolExecutor() as ex:
        f1 = ex.submit(chat_ollama, MODEL_IDS["LLaMA 3"], f"Resuma o problema com exatid√£o em 2 linhas: {msg}", {"num_predict": 100}, 15)
        llama = f1.result(timeout=15)
        f2 = ex.submit(chat_ollama, MODEL_IDS["CodeGemma 7B"], f"Gere c√≥digo curto e funcional baseado em: {llama}", {"num_predict": 180}, 20)
        code = f2.result(timeout=20)
        phi = chat_ollama(MODEL_IDS["Phi-3"], f"Explique o c√≥digo abaixo em at√© 5 linhas e cite 2 melhorias:\n{code}",
                          {"num_predict": 160}, 20)
    return phi

# Escola: duas respostas r√°pidas em paralelo e combinadas
def gerar_resposta_duo_escola(msg: str) -> str:
    with concurrent.futures.ThreadPoolExecutor() as ex:
        f1 = ex.submit(chat_ollama, MODEL_IDS["Phi-3"], f"Explique didaticamente para aluno do EM: {msg}", {"num_predict": 160}, 20)
        f2 = ex.submit(chat_ollama, MODEL_IDS["LLaMA 3"], f"D√™ 1 exemplo pr√°tico simples sobre: {msg}", {"num_predict": 140}, 20)
        r1 = f1.result(timeout=20)
        r2 = f2.result(timeout=20)
    return f"{r1}\n\nüìò Exemplo:\n{r2}"

# Pipeline de Trabalho/Professor (WORK): LLaMA 3 -> CodeGemma -> Phi-3 com prompt espec√≠fico por modo
def gerar_resposta_work(modo: str, msg: str) -> str:
    # 1) LLaMA 3: estrutura o plano/ideias iniciais
    if modo == "üë®‚Äçüè´ Professor":
        p1 = (f"Crie um plano de resposta educacional claro: objetivo, t√≥picos em bullets e sequ√™ncia did√°tica. "
              f"Foque em clareza para ensino m√©dio.\nTema: {msg}")
    elif modo == "üé® Designer":
        p1 = (f"Fa√ßa um esbo√ßo textual de ideias visuais (layout, hierarquia, tipografia, cores) em bullets. "
              f"Seja espec√≠fico e pr√°tico para UI/UX.\nBriefing: {msg}")
    elif modo == "üíª Codificador":
        p1 = (f"Liste requisitos e estrat√©gia de implementa√ß√£o em bullets curtos. "
              f"Em seguida, proponha uma arquitetura m√≠nima.\nTarefa: {msg}")
    elif modo == "üß© Estrat√©gias":
        p1 = (f"Defina uma estrat√©gia objetiva com metas, a√ß√µes t√°ticas e m√©tricas (bullets). "
              f"Seja sucinto e acion√°vel.\nDesafio: {msg}")
    else:
        p1 = f"Organize um plano objetivo em bullets para: {msg}"

    etapa1 = chat_ollama(MODEL_IDS["LLaMA 3"], p1, {"num_predict": 160}, 20)

    # 2) CodeGemma: converte o plano em material aplic√°vel (c√≥digo/estrutura/propostas)
    if modo == "üíª Codificador":
        p2 = (f"Com base no plano abaixo, gere um c√≥digo curto e funcional (‚â§50 linhas) "
              f"e adicione coment√°rios essenciais. Plano:\n{etapa1}")
    elif modo == "üé® Designer":
        p2 = (f"Transforme o plano abaixo em entreg√°veis textuais (componentes, se√ß√µes, grid, guidelines). "
              f"Inclua 1 estrutura de p√°gina em bullets. Plano:\n{etapa1}")
    elif modo == "üë®‚Äçüè´ Professor":
        p2 = (f"Transforme o plano abaixo em um mini-plano de aula + 3 exerc√≠cios com gabarito. "
              f"Plano:\n{etapa1}")
    else:  # Estrat√©gias
        p2 = (f"Converta o plano abaixo em um playbook pr√°tico com a√ß√µes sequenciadas (1-2 semanas). "
              f"Plano:\n{etapa1}")

    etapa2 = chat_ollama(MODEL_IDS["CodeGemma 7B"], p2, {"num_predict": 240}, 25)

    # 3) Phi-3: refina e entrega instru√ß√µes finais curtas e claras
    p3 = (f"Refine o material abaixo em at√© 8 linhas, com passos claros e recomenda√ß√µes finais objetivas:\n{etapa2}")
    etapa3 = chat_ollama(MODEL_IDS["Phi-3"], p3, {"num_predict": 160}, 20)

    return etapa3

# ==================================
# GERADOR PADR√ÉO POR MODO
# ==================================
def gerar_resposta(modelo_display: str | None, modo: str, msg: str) -> str:
    # modos especiais com pipelines
    if modo == "‚òÑÔ∏è Ultra (Beta)":
        return gerar_resposta_ultra(msg)
    if modo == "üè´ Escola":
        return gerar_resposta_duo_escola(msg)
    if MODE_LIMITS.get(modo) == ["WORK"]:
        return gerar_resposta_work(modo, msg)

    # modo Escritor turbo (Phi-3)
    if modo == "‚úçÔ∏è Escritor":
        prompt = (
            "Crie um texto de ALTA QUALIDADE com 5‚Äì10 linhas, direto ao ponto, coeso e sem enrola√ß√£o.\n\n"
            f"Tema: {msg}"
        )
        return chat_ollama(
            MODEL_IDS["Phi-3"], prompt,
            {"temperature": 0.4, "num_predict": 200, "top_p": 0.8, "repeat_penalty": 1.1},
            25
        )

    # Pro Beta (CodeGemma)
    if modo == "üíé Pro (Beta)":
        prompt = f"{MODE_PROMPTS[modo]}\n\n{msg}"
        return chat_ollama(MODEL_IDS["CodeGemma 7B"], prompt, {"num_predict": 220}, 25)

    # Normal (LLaMA 3 turbo)
    if modo == "üîµ Normal":
        prompt = f"{MODE_PROMPTS[modo]}\n\nPergunta: {msg}"
        return chat_ollama(MODEL_IDS["LLaMA 3"], prompt, {"num_predict": 180}, 25)

    # Demais modos com seu modelo fixo/selecionado
    prompt = f"{MODE_PROMPTS.get(modo, 'Seja claro e objetivo.')}\n\nPergunta: {msg}"
    if modelo_display is None:
        # fallback seguro
        modelo_display = "LLaMA 3"
    model_id = MODEL_IDS[modelo_display]
    return chat_ollama(model_id, prompt, {"num_predict": 140}, 20)

# ==============================
# LOGIN / SIGN-UP / CONVIDADO
# ==============================
if "usuario" not in st.session_state: st.session_state.usuario = None
if "plano" not in st.session_state: st.session_state.plano = None

if st.session_state.usuario is None:
    st.title("ü§ñ PrimeBud Turbo 1.0")
    st.caption("Feito por: **Primorix Studios**")
    st.link_button("üåê Ver no GitHub", GITHUB_URL)
    st.markdown("---")

    aba = st.tabs(["Entrar", "Criar conta", "Convidado (Ultra)"])

    with aba[0]:
        u = st.text_input("Usu√°rio")
        p = st.text_input("Senha", type="password")
        if st.button("Entrar"):
            db = st.session_state.usuarios
            if u in db and db[u]["senha"] == p:
                st.session_state.usuario = u
                st.session_state.plano = db[u]["plano"]
                st.success("Login bem-sucedido!")
                st.rerun()
            else:
                st.error("Usu√°rio ou senha incorretos.")

    with aba[1]:
        novo_u = st.text_input("Novo usu√°rio")
        nova_s = st.text_input("Nova senha", type="password")
        plano_escolhido = st.selectbox("Plano inicial", ["Free", "Pro", "Ultra", "Trabalho", "Professor"])
        if st.button("Criar conta"):
            db = st.session_state.usuarios
            if novo_u in db:
                st.warning("Usu√°rio j√° existe.")
            else:
                db[novo_u] = {"senha": nova_s, "plano": plano_escolhido}
                st.session_state.usuario = novo_u
                st.session_state.plano = plano_escolhido
                st.success(f"Conta criada e login autom√°tico como {novo_u} ({plano_escolhido}).")
                st.rerun()

    with aba[2]:
        if st.button("Entrar como convidado (Plano Ultra)"):
            st.session_state.usuario = "Convidado"
            st.session_state.plano = "Ultra"
            st.success("Entrou como convidado ‚Äî Plano Ultra liberado.")
            st.rerun()
    st.stop()

usuario = st.session_state.usuario
plano = st.session_state.plano

# ==============================
# M√öLTIPLOS CHATS
# ==============================
if "chats" not in st.session_state: st.session_state.chats = []
if "chat_atual" not in st.session_state:
    st.session_state.chat_atual = 0
    st.session_state.chats.append({"nome": "Chat 1", "historico": []})

def novo_chat():
    n = len(st.session_state.chats) + 1
    st.session_state.chats.append({"nome": f"Chat {n}", "historico": []})
    st.session_state.chat_atual = len(st.session_state.chats) - 1
    st.success(f"Novo chat criado ‚Äî Chat {n}")
    st.rerun()

# ==============================
# SIDEBAR (estilo ChatGPT)
# ==============================
with st.sidebar:
    st.title(f"ü§ñ PrimeBud ‚Äî {usuario}")
    st.markdown(f"**Plano:** {plano}")
    st.warning("üíæ Conversas **n√£o s√£o salvas permanentemente**.")
    if st.button("‚ûï Novo chat"):
        novo_chat()

    # Lista de chats
    chats = [c["nome"] for c in st.session_state.chats]
    idx = st.radio("Seus chats:", list(range(len(chats))), format_func=lambda i: chats[i])
    st.session_state.chat_atual = idx

    # Modos por plano (incluindo a nova assinatura Trabalho e Professor)
    modos_por_plano = {
        "Free": ["üí¨ Mini", "üçÉ Econ√¥mico", "‚úçÔ∏è Escritor", "üè´ Escola"],
        "Pro": ["‚ö° Flash", "üîµ Normal", "üíé Pro (Beta)", "üçÉ Econ√¥mico", "‚úçÔ∏è Escritor", "üè´ Escola"],
        "Ultra": ["‚ö° Flash", "üîµ Normal", "üíé Pro (Beta)", "üçÉ Econ√¥mico", "üí¨ Mini",
                  "‚òÑÔ∏è Ultra (Beta)", "‚úçÔ∏è Escritor", "üè´ Escola",
                  "üë®‚Äçüè´ Professor", "üé® Designer", "üíª Codificador", "üß© Estrat√©gias"],
        "Trabalho": ["üë®‚Äçüè´ Professor", "üé® Designer", "üíª Codificador", "üß© Estrat√©gias",
                     "‚úçÔ∏è Escritor", "üè´ Escola"],
        "Professor": ["üë®‚Äçüè´ Professor", "üè´ Escola", "‚úçÔ∏è Escritor"],  # assinatura focada
    }

    # seguran√ßa: se plano desconhecido, cai no Ultra
    lista_modos = modos_por_plano.get(plano, modos_por_plano["Ultra"])
    modo = st.radio("Modo:", lista_modos)
    st.markdown(f"**Descri√ß√£o:** {MODOS_DESC.get(modo, 'Modo customizado.')}")

    # Mostrar/ocultar sele√ß√£o de LLM dependendo do modo
    allowed = MODE_LIMITS.get(modo, ["LLaMA 3"])
    if "PIPELINE" in allowed:
        st.info("‚òÑÔ∏è Ultra (Beta) usa automaticamente: LLaMA 3 ‚Üí CodeGemma 7B ‚Üí Phi-3.")
        modelo_display = None
    elif "DUO" in allowed:
        st.info("üè´ Escola usa automaticamente: Phi-3 + LLaMA 3.")
        modelo_display = None
    elif "WORK" in allowed:
        st.info("Assinatura Trabalho (pipeline): LLaMA 3 ‚Üí CodeGemma 7B ‚Üí Phi-3.")
        modelo_display = None
    else:
        if len(allowed) == 1:
            modelo_display = allowed[0]
            st.success(f"LLM deste modo: **{modelo_display}**")
        else:
            modelo_display = st.selectbox("LLM:", allowed)
            st.success(f"LLM ativo: **{modelo_display}**")

# ==============================
# √ÅREA PRINCIPAL DO CHAT
# ==============================
chat = st.session_state.chats[st.session_state.chat_atual]
st.markdown(f"### üí¨ {chat['nome']}")

st.markdown("""
<style>
.chat-container{max-width:900px;margin:auto;}
.user-bubble{background:#2b313e;color:#fff;padding:10px;border-radius:10px;margin:6px 0;}
.bot-bubble{background:#ececf1;color:#000;padding:10px;border-radius:10px;margin:6px 0;}
</style>
""", unsafe_allow_html=True)
st.markdown("<div class='chat-container'>", unsafe_allow_html=True)

# Render hist√≥rico
for m in chat["historico"]:
    bubble = "user-bubble" if m["autor"] == "user" else "bot-bubble"
    st.markdown(f"<div class='{bubble}'><b>{'Voc√™' if m['autor']=='user' else 'PrimeBud'}:</b> {m['texto']}</div>", unsafe_allow_html=True)

# Entrada
msg = st.chat_input("Envie uma mensagem‚Ä¶")
if msg:
    chat["historico"].append({"autor": "user", "texto": msg})
    with st.spinner("Processando‚Ä¶ (m√°x ~30 s)"):
        resposta = gerar_resposta(modelo_display, modo, msg)
    chat["historico"].append({"autor": "bot", "texto": resposta})
    st.rerun()

st.markdown("</div>", unsafe_allow_html=True)

