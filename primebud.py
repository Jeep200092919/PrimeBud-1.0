import streamlit as st
import requests
import json
import concurrent.futures

# ==============================
# CONFIGURAÃ‡Ã•ES GERAIS
# ==============================
st.set_page_config(page_title="PrimeBud Turbo 1.0", page_icon="ğŸ¤–", layout="wide")

OLLAMA_URL = "http://localhost:11434/api/chat"
GITHUB_URL = "https://github.com/Jeep200092919/PrimeBud-Turbo-1.0"

# ==============================
# MODELOS (LLaMA 3 no lugar do Mistral)
# ==============================
MODEL_IDS = {
    "LLaMA 3": "llama3",
    "CodeGemma 7B": "codegemma:7b",
    "Phi-3": "phi3",
}

# ==============================
# BASE DE USUÃRIOS (comeÃ§a simples)
# ==============================
if "usuarios" not in st.session_state:
    st.session_state.usuarios = {"teste": {"senha": "0000", "plano": "Free"}}

# ==============================
# DESCRIÃ‡Ã•ES DE MODO
# ==============================
MODOS_DESC = {
    "âš¡ Flash": "Respostas curtas e instantÃ¢neas (LLaMA 3).",
    "ğŸ”µ Normal": "Respostas equilibradas e naturais (LLaMA 3, turbo).",
    "ğŸƒ EconÃ´mico": "Respostas curtas e otimizadas (LLaMA 3).",
    "ğŸ’¬ Mini": "Conversas leves e sem cÃ³digo (LLaMA 3).",
    "ğŸ’ Pro (Beta)": "CÃ³digo + explicaÃ§Ã£o tÃ©cnica curta (CodeGemma 7B).",
    "â˜„ï¸ Ultra (Beta)": "Pipeline turbo (LLaMA 3 â†’ CodeGemma 7B â†’ Phi-3).",
    "âœï¸ Escritor": "Textos criativos (5â€“10 linhas) muito rÃ¡pidos (Phi-3).",
    "ğŸ« Escola": "Ajuda escolar (1Âºâ€“3Âº EM) â€” dupla rÃ¡pida (Phi-3 + LLaMA 3).",
    # Modos profissionais (Assinatura Trabalho / Ultra / Professor)
    "ğŸ‘¨â€ğŸ« Professor": "Explica conteÃºdos, cria plano de aula e exercÃ­cios (pipeline LLaMA 3 â†’ CodeGemma â†’ Phi-3).",
    "ğŸ¨ Designer": "Ideias visuais, UI/UX, prompts de imagem e estrutura de layout (pipeline LLaMA 3 â†’ CodeGemma â†’ Phi-3).",
    "ğŸ’» Codificador": "Gera, corrige e explica cÃ³digo limpo (pipeline LLaMA 3 â†’ CodeGemma â†’ Phi-3).",
    "ğŸ§© EstratÃ©gias": "EstratÃ©gias de negÃ³cio/marketing/tech claras e acionÃ¡veis (pipeline LLaMA 3 â†’ CodeGemma â†’ Phi-3).",
}

# ==============================
# LIMITES DE MODELOS POR MODO
# ==============================
MODE_LIMITS = {
    "âš¡ Flash": ["LLaMA 3"],
    "ğŸ”µ Normal": ["LLaMA 3"],
    "ğŸƒ EconÃ´mico": ["LLaMA 3"],
    "ğŸ’¬ Mini": ["LLaMA 3"],
    "ğŸ’ Pro (Beta)": ["CodeGemma 7B"],
    "â˜„ï¸ Ultra (Beta)": ["PIPELINE"],  # LLaMA3 -> CodeGemma -> Phi-3
    "âœï¸ Escritor": ["Phi-3"],
    "ğŸ« Escola": ["DUO"],             # Phi-3 + LLaMA 3
    # Modos Trabalho / Professor usam pipeline WORK dedicada
    "ğŸ‘¨â€ğŸ« Professor": ["WORK"],
    "ğŸ¨ Designer": ["WORK"],
    "ğŸ’» Codificador": ["WORK"],
    "ğŸ§© EstratÃ©gias": ["WORK"],
}

# ==============================
# PROMPTS DE ESTILO (objetivos = maior velocidade)
# ==============================
MODE_PROMPTS = {
    "âš¡ Flash": "Responda em atÃ© 2 frases, direto e sem rodeios.",
    "ğŸ”µ Normal": "Explique de forma clara e breve (atÃ© 6 linhas).",
    "ğŸƒ EconÃ´mico": "MÃ¡x. 3 linhas, direto ao ponto.",
    "ğŸ’¬ Mini": "Converse de modo leve e simples. NÃ£o gere cÃ³digo.",
    "ğŸ’ Pro (Beta)": "ForneÃ§a o cÃ³digo e breve explicaÃ§Ã£o tÃ©cnica (â‰¤5 linhas).",
    "âœï¸ Escritor": "Crie um texto coeso e criativo de 5â€“10 linhas.",
    "ğŸ« Escola": "Explique e resolva dÃºvidas do Ensino MÃ©dio de forma didÃ¡tica e rÃ¡pida.",
}

# ==============================
# FUNÃ‡ÃƒO BÃSICA DE CHAMADA AO OLLAMA (com timeout e stream=False)
# ==============================
def chat_ollama(model: str, prompt: str, options: dict | None = None, timeout: int = 30) -> str:
    payload = {
        "model": model,
        "stream": False,
        "messages": [
            {"role": "system", "content": "VocÃª Ã© o PrimeBud. Seja objetivo, claro e Ãºtil."},
            {"role": "user", "content": prompt}
        ]
    }
    if options:
        payload["options"] = options

    r = requests.post(OLLAMA_URL, json=payload, timeout=timeout)
    try:
        data = r.json()
    except json.JSONDecodeError:
        # fallback caso venham mÃºltiplas linhas JSON no buffer
        data = json.loads(r.text.strip().split("\n")[0])
    return data["message"]["content"]

# ==============================
# PIPELINES OTIMIZADAS (Turbo)
# ==============================

# Ultra: LLaMA 3 (resumo/entendimento) -> CodeGemma (cÃ³digo) -> Phi-3 (refino/explicaÃ§Ã£o)
def gerar_resposta_ultra(msg: str) -> str:
    with concurrent.futures.ThreadPoolExecutor() as ex:
        f1 = ex.submit(chat_ollama, MODEL_IDS["LLaMA 3"], f"Resuma o problema com exatidÃ£o em 2 linhas: {msg}", {"num_predict": 100}, 15)
        llama = f1.result(timeout=15)
        f2 = ex.submit(chat_ollama, MODEL_IDS["CodeGemma 7B"], f"Gere cÃ³digo curto e funcional baseado em: {llama}", {"num_predict": 180}, 20)
        code = f2.result(timeout=20)
        phi = chat_ollama(MODEL_IDS["Phi-3"], f"Explique o cÃ³digo abaixo em atÃ© 5 linhas e cite 2 melhorias:\n{code}",
                          {"num_predict": 160}, 20)
    return phi

# Escola: duas respostas rÃ¡pidas em paralelo e combinadas
def gerar_resposta_duo_escola(msg: str) -> str:
    with concurrent.futures.ThreadPoolExecutor() as ex:
        f1 = ex.submit(chat_ollama, MODEL_IDS["Phi-3"], f"Explique didaticamente para aluno do EM: {msg}", {"num_predict": 160}, 20)
        f2 = ex.submit(chat_ollama, MODEL_IDS["LLaMA 3"], f"DÃª 1 exemplo prÃ¡tico simples sobre: {msg}", {"num_predict": 140}, 20)
        r1 = f1.result(timeout=20)
        r2 = f2.result(timeout=20)
    return f"{r1}\n\nğŸ“˜ Exemplo:\n{r2}"

# Pipeline de Trabalho/Professor (WORK): LLaMA 3 -> CodeGemma -> Phi-3 com prompt especÃ­fico por modo
def gerar_resposta_work(modo: str, msg: str) -> str:
    # 1) LLaMA 3: estrutura o plano/ideias iniciais
    if modo == "ğŸ‘¨â€ğŸ« Professor":
        p1 = (f"Crie um plano de resposta educacional claro: objetivo, tÃ³picos em bullets e sequÃªncia didÃ¡tica. "
              f"Foque em clareza para ensino mÃ©dio.\nTema: {msg}")
    elif modo == "ğŸ¨ Designer":
        p1 = (f"FaÃ§a um esboÃ§o textual de ideias visuais (layout, hierarquia, tipografia, cores) em bullets. "
              f"Seja especÃ­fico e prÃ¡tico para UI/UX.\nBriefing: {msg}")
    elif modo == "ğŸ’» Codificador":
        p1 = (f"Liste requisitos e estratÃ©gia de implementaÃ§Ã£o em bullets curtos. "
              f"Em seguida, proponha uma arquitetura mÃ­nima.\nTarefa: {msg}")
    elif modo == "ğŸ§© EstratÃ©gias":
        p1 = (f"Defina uma estratÃ©gia objetiva com metas, aÃ§Ãµes tÃ¡ticas e mÃ©tricas (bullets). "
              f"Seja sucinto e acionÃ¡vel.\nDesafio: {msg}")
    else:
        p1 = f"Organize um plano objetivo em bullets para: {msg}"

    etapa1 = chat_ollama(MODEL_IDS["LLaMA 3"], p1, {"num_predict": 160}, 20)

    # 2) CodeGemma: converte o plano em material aplicÃ¡vel (cÃ³digo/estrutura/propostas)
    if modo == "ğŸ’» Codificador":
        p2 = (f"Com base no plano abaixo, gere um cÃ³digo curto e funcional (â‰¤50 linhas) "
              f"e adicione comentÃ¡rios essenciais. Plano:\n{etapa1}")
    elif modo == "ğŸ¨ Designer":
        p2 = (f"Transforme o plano abaixo em entregÃ¡veis textuais (componentes, seÃ§Ãµes, grid, guidelines). "
              f"Inclua 1 estrutura de pÃ¡gina em bullets. Plano:\n{etapa1}")
    elif modo == "ğŸ‘¨â€ğŸ« Professor":
        p2 = (f"Transforme o plano abaixo em um mini-plano de aula + 3 exercÃ­cios com gabarito. "
              f"Plano:\n{etapa1}")
    else:  # EstratÃ©gias
        p2 = (f"Converta o plano abaixo em um playbook prÃ¡tico com aÃ§Ãµes sequenciadas (1-2 semanas). "
              f"Plano:\n{etapa1}")

    etapa2 = chat_ollama(MODEL_IDS["CodeGemma 7B"], p2, {"num_predict": 240}, 25)

    # 3) Phi-3: refina e entrega instruÃ§Ãµes finais curtas e claras
    p3 = (f"Refine o material abaixo em atÃ© 8 linhas, com passos claros e recomendaÃ§Ãµes finais objetivas:\n{etapa2}")
    etapa3 = chat_ollama(MODEL_IDS["Phi-3"], p3, {"num_predict": 160}, 20)

    return etapa3

# ==================================
# GERADOR PADRÃƒO POR MODO
# ==================================
def gerar_resposta(modelo_display: str | None, modo: str, msg: str) -> str:
    # modos especiais com pipelines
    if modo == "â˜„ï¸ Ultra (Beta)":
        return gerar_resposta_ultra(msg)
    if modo == "ğŸ« Escola":
        return gerar_resposta_duo_escola(msg)
    if MODE_LIMITS.get(modo) == ["WORK"]:
        return gerar_resposta_work(modo, msg)

    # modo Escritor turbo (Phi-3)
    if modo == "âœï¸ Escritor":
        prompt = (
            "Crie um texto de ALTA QUALIDADE com 5â€“10 linhas, direto ao ponto, coeso e sem enrolaÃ§Ã£o.\n\n"
            f"Tema: {msg}"
        )
        return chat_ollama(
            MODEL_IDS["Phi-3"], prompt,
            {"temperature": 0.4, "num_predict": 200, "top_p": 0.8, "repeat_penalty": 1.1},
            25
        )

    # Pro Beta (CodeGemma)
    if modo == "ğŸ’ Pro (Beta)":
        prompt = f"{MODE_PROMPTS[modo]}\n\n{msg}"
        return chat_ollama(MODEL_IDS["CodeGemma 7B"], prompt, {"num_predict": 220}, 25)

    # Normal (LLaMA 3 turbo)
    if modo == "ğŸ”µ Normal":
        prompt = f"{MODE_PROMPTS[modo]}\n\nPergunta: {msg}"
        return chat_ollama(MODEL_IDS["LLaMA 3"], prompt, {"num_predict": 180}, 25)

    # Demais modos com seu modelo fixo/selecionado
    prompt = f"{MODE_PROMPTS.get(modo, 'Seja claro e objetivo.')}\n\nPergunta: {msg}"
    if modelo_display is None:
        # fallback seguro
        modelo_display = "LLaMA 3"
    model_id = MODEL_IDS[modelo_display]
    return chat_ollama(model_id, prompt, {"num_predict": 140}, 20)

# ==============================
# LOGIN / SIGN-UP / CONVIDADO
# ==============================
if "usuario" not in st.session_state: st.session_state.usuario = None
if "plano" not in st.session_state: st.session_state.plano = None

if st.session_state.usuario is None:
    st.title("ğŸ¤– PrimeBud Turbo 1.0")
    st.caption("Feito por: **Primorix Studios**")
    st.link_button("ğŸŒ Ver no GitHub", GITHUB_URL)
    st.markdown("---")

    aba = st.tabs(["Entrar", "Criar conta", "Convidado (Ultra)"])

    with aba[0]:
        u = st.text_input("UsuÃ¡rio")
        p = st.text_input("Senha", type="password")
        if st.button("Entrar"):
            db = st.session_state.usuarios
            if u in db and db[u]["senha"] == p:
                st.session_state.usuario = u
                st.session_state.plano = db[u]["plano"]
                st.success("Login bem-sucedido!")
                st.rerun()
            else:
                st.error("UsuÃ¡rio ou senha incorretos.")

    with aba[1]:
        novo_u = st.text_input("Novo usuÃ¡rio")
        nova_s = st.text_input("Nova senha", type="password")
        plano_escolhido = st.selectbox("Plano inicial", ["Free", "Pro", "Ultra", "Trabalho", "Professor"])
        if st.button("Criar conta"):
            db = st.session_state.usuarios
            if novo_u in db:
                st.warning("UsuÃ¡rio jÃ¡ existe.")
            else:
                db[novo_u] = {"senha": nova_s, "plano": plano_escolhido}
                st.session_state.usuario = novo_u
                st.session_state.plano = plano_escolhido
                st.success(f"Conta criada e login automÃ¡tico como {novo_u} ({plano_escolhido}).")
                st.rerun()

    with aba[2]:
        if st.button("Entrar como convidado (Plano Ultra)"):
            st.session_state.usuario = "Convidado"
            st.session_state.plano = "Ultra"
            st.success("Entrou como convidado â€” Plano Ultra liberado.")
            st.rerun()
    st.stop()

usuario = st.session_state.usuario
plano = st.session_state.plano

# ==============================
# MÃšLTIPLOS CHATS
# ==============================
if "chats" not in st.session_state: st.session_state.chats = []
if "chat_atual" not in st.session_state:
    st.session_state.chat_atual = 0
    st.session_state.chats.append({"nome": "Chat 1", "historico": []})

def novo_chat():
    n = len(st.session_state.chats) + 1
    st.session_state.chats.append({"nome": f"Chat {n}", "historico": []})
    st.session_state.chat_atual = len(st.session_state.chats) - 1
    st.success(f"Novo chat criado â€” Chat {n}")
    st.rerun()

# ==============================
# SIDEBAR (estilo ChatGPT)
# ==============================
with st.sidebar:
    st.title(f"ğŸ¤– PrimeBud â€” {usuario}")
    st.markdown(f"**Plano:** {plano}")
    st.warning("ğŸ’¾ Conversas **nÃ£o sÃ£o salvas permanentemente**.")
    if st.button("â• Novo chat"):
        novo_chat()

    # Lista de chats
    chats = [c["nome"] for c in st.session_state.chats]
    idx = st.radio("Seus chats:", list(range(len(chats))), format_func=lambda i: chats[i])
    st.session_state.chat_atual = idx

    # Modos por plano (incluindo a nova assinatura Trabalho e Professor)
    modos_por_plano = {
        "Free": ["ğŸ’¬ Mini", "ğŸƒ EconÃ´mico", "âœï¸ Escritor", "ğŸ« Escola"],
        "Pro": ["âš¡ Flash", "ğŸ”µ Normal", "ğŸ’ Pro (Beta)", "ğŸƒ EconÃ´mico", "âœï¸ Escritor", "ğŸ« Escola"],
        "Ultra": ["âš¡ Flash", "ğŸ”µ Normal", "ğŸ’ Pro (Beta)", "ğŸƒ EconÃ´mico", "ğŸ’¬ Mini",
                  "â˜„ï¸ Ultra (Beta)", "âœï¸ Escritor", "ğŸ« Escola",
                  "ğŸ‘¨â€ğŸ« Professor", "ğŸ¨ Designer", "ğŸ’» Codificador", "ğŸ§© EstratÃ©gias"],
        "Trabalho": ["ğŸ‘¨â€ğŸ« Professor", "ğŸ¨ Designer", "ğŸ’» Codificador", "ğŸ§© EstratÃ©gias",
                     "âœï¸ Escritor", "ğŸ« Escola"],
        "Professor": ["ğŸ‘¨â€ğŸ« Professor", "ğŸ« Escola", "âœï¸ Escritor"],  # assinatura focada
    }

    # seguranÃ§a: se plano desconhecido, cai no Ultra
    lista_modos = modos_por_plano.get(plano, modos_por_plano["Ultra"])
    modo = st.radio("Modo:", lista_modos)
    st.markdown(f"**DescriÃ§Ã£o:** {MODOS_DESC.get(modo, 'Modo customizado.')}")

    # Mostrar/ocultar seleÃ§Ã£o de LLM dependendo do modo
    allowed = MODE_LIMITS.get(modo, ["LLaMA 3"])
    if "PIPELINE" in allowed:
        st.info("â˜„ï¸ Ultra (Beta) usa automaticamente: LLaMA 3 â†’ CodeGemma 7B â†’ Phi-3.")
        modelo_display = None
    elif "DUO" in allowed:
        st.info("ğŸ« Escola usa automaticamente: Phi-3 + LLaMA 3.")
        modelo_display = None
    elif "WORK" in allowed:
        st.info("Assinatura Trabalho (pipeline): LLaMA 3 â†’ CodeGemma 7B â†’ Phi-3.")
        modelo_display = None
    else:
        if len(allowed) == 1:
            modelo_display = allowed[0]
            st.success(f"LLM deste modo: **{modelo_display}**")
        else:
            modelo_display = st.selectbox("LLM:", allowed)
            st.success(f"LLM ativo: **{modelo_display}**")

# ==============================
# ÃREA PRINCIPAL DO CHAT
# ==============================
chat = st.session_state.chats[st.session_state.chat_atual]
st.markdown(f"### ğŸ’¬ {chat['nome']}")

st.markdown("""
<style>
.chat-container{max-width:900px;margin:auto;}
.user-bubble{background:#2b313e;color:#fff;padding:10px;border-radius:10px;margin:6px 0;}
.bot-bubble{background:#ececf1;color:#000;padding:10px;border-radius:10px;margin:6px 0;}
</style>
""", unsafe_allow_html=True)
st.markdown("<div class='chat-container'>", unsafe_allow_html=True)

# Render histÃ³rico
for m in chat["historico"]:
    bubble = "user-bubble" if m["autor"] == "user" else "bot-bubble"
    st.markdown(f"<div class='{bubble}'><b>{'VocÃª' if m['autor']=='user' else 'PrimeBud'}:</b> {m['texto']}</div>", unsafe_allow_html=True)

# Entrada
msg = st.chat_input("Envie uma mensagemâ€¦")
if msg:
    chat["historico"].append({"autor": "user", "texto": msg})
    with st.spinner("Processandoâ€¦ (mÃ¡x ~30 s)"):
        resposta = gerar_resposta(modelo_display, modo, msg)
    chat["historico"].append({"autor": "bot", "texto": resposta})
    st.rerun()

st.markdown("</div>", unsafe_allow_html=True)

