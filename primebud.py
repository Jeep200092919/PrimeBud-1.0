import streamlit as st
import requests
import json
import concurrent.futures
import os

# ==============================
# CONFIGURAÃ‡Ã•ES INICIAIS
# ==============================
st.set_page_config(page_title="PrimeBud Turbo 1.1", page_icon="ğŸ¤–", layout="wide")

# URLs principais
OLLAMA_URL = "http://localhost:11434/api/chat"
GROQ_URL = "https://api.groq.com/openai/v1/chat/completions"

# Carrega secrets (Streamlit Cloud) ou variÃ¡veis locais
GROQ_API_KEY = st.secrets.get("GROQ_API_KEY", os.getenv("GROQ_API_KEY", ""))
GROQ_MODEL = st.secrets.get("GROQ_MODEL", os.getenv("GROQ_MODEL", "llama3-8b-8192"))

GITHUB_URL = "https://github.com/Jeep200092919/PrimeBud-1.0"

# ==============================
# DETECÃ‡ÃƒO AUTOMÃTICA DE BACKEND
# ==============================
def usar_groq() -> bool:
    """Verifica se hÃ¡ uma API key da Groq disponÃ­vel."""
    return bool(GROQ_API_KEY)

def _map_options_for_openai_like(options: dict | None) -> dict:
    options = options or {}
    return {
        "temperature": options.get("temperature", 0.6),
        "top_p": options.get("top_p", 0.9),
        "max_tokens": options.get("num_predict", 400),
    }

# ==============================
# CHAMADAS DE IA
# ==============================
def chat_api(model: str, prompt: str, options: dict | None = None, timeout: int = 60) -> str:
    """
    Usa Groq (LLaMA3 online) se houver chave; senÃ£o usa Ollama local.
    MantÃ©m compatibilidade com todos os modos do PrimeBud.
    """
    # Se houver chave da Groq â†’ usa o modelo hospedado
    if usar_groq():
        payload = {
            "model": GROQ_MODEL,
            "messages": [
                {"role": "system", "content": "VocÃª Ã© o PrimeBud Turbo 1.1 â€” seja claro, rÃ¡pido e Ãºtil."},
                {"role": "user", "content": prompt},
            ],
            "stream": False,
            **_map_options_for_openai_like(options),
        }
        headers = {"Authorization": f"Bearer {GROQ_API_KEY}"}
        r = requests.post(GROQ_URL, headers=headers, json=payload, timeout=timeout)
        data = r.json()
        return data["choices"][0]["message"]["content"]

    # Caso contrÃ¡rio, usa o Ollama local
    payload = {
        "model": model,
        "stream": False,
        "messages": [
            {"role": "system", "content": "VocÃª Ã© o PrimeBud Turbo. Seja claro, rÃ¡pido e Ãºtil."},
            {"role": "user", "content": prompt},
        ],
    }
    if options:
        payload["options"] = options
    r = requests.post(OLLAMA_URL, json=payload, timeout=timeout)
    try:
        data = r.json()
    except json.JSONDecodeError:
        data = json.loads(r.text.strip().split("\n")[0])
    return data["message"]["content"]

# ==============================
# MODELOS E MODOS
# ==============================
MODEL_IDS = {
    "LLaMA 3": "llama3-8b-8192",
    "CodeGemma 7B": "llama3-8b-8192",
    "Phi-3": "llama3-8b-8192",
}

MODOS_DESC = {
    "âš¡ Flash": "Respostas curtas e instantÃ¢neas (LLaMA 3).",
    "ğŸ”µ Normal": "Respostas equilibradas e naturais (LLaMA 3).",
    "ğŸƒ EconÃ´mico": "Respostas curtas e otimizadas (LLaMA 3).",
    "ğŸ’¬ Mini": "Conversas leves e simples.",
    "ğŸ’ Pro (Beta)": "CÃ³digo + breve explicaÃ§Ã£o (LLaMA 3).",
    "â˜„ï¸ Ultra (Beta)": "Processamento turbo em mÃºltiplas etapas.",
    "âœï¸ Escritor": "Textos criativos de 5â€“10 linhas.",
    "ğŸ« Escola": "ExplicaÃ§Ãµes didÃ¡ticas do Ensino MÃ©dio.",
    "ğŸ‘¨â€ğŸ« Professor": "ExplicaÃ§Ãµes completas e exemplos prÃ¡ticos.",
    "ğŸ¨ Designer": "Ideias visuais e criativas.",
    "ğŸ’» Codificador": "CÃ³digo limpo com explicaÃ§Ã£o curta.",
    "ğŸ§© EstratÃ©gias": "Planos de aÃ§Ã£o objetivos e mensurÃ¡veis.",
}

MODE_LIMITS = {m: ["LLaMA 3"] for m in MODOS_DESC.keys()}

# ==============================
# FUNÃ‡Ã•ES DE RESPOSTA POR MODO
# ==============================
def gerar_resposta(modo: str, msg: str) -> str:
    base_prompt = MODOS_DESC.get(modo, "Seja direto e Ãºtil.")
    full_prompt = f"{base_prompt}\n\n{msg}"

    # ParÃ¢metros distintos simulando "velocidades"
    config = {
        "âš¡ Flash": {"temperature": 0.3, "num_predict": 100},
        "ğŸ”µ Normal": {"temperature": 0.5, "num_predict": 220},
        "ğŸƒ EconÃ´mico": {"temperature": 0.4, "num_predict": 120},
        "ğŸ’¬ Mini": {"temperature": 0.6, "num_predict": 150},
        "ğŸ’ Pro (Beta)": {"temperature": 0.4, "num_predict": 240},
        "â˜„ï¸ Ultra (Beta)": {"temperature": 0.6, "num_predict": 300},
        "âœï¸ Escritor": {"temperature": 0.9, "num_predict": 250},
        "ğŸ« Escola": {"temperature": 0.6, "num_predict": 250},
        "ğŸ‘¨â€ğŸ« Professor": {"temperature": 0.4, "num_predict": 300},
        "ğŸ¨ Designer": {"temperature": 0.9, "num_predict": 220},
        "ğŸ’» Codificador": {"temperature": 0.2, "num_predict": 280},
        "ğŸ§© EstratÃ©gias": {"temperature": 0.6, "num_predict": 260},
    }

    opt = config.get(modo, {"temperature": 0.5, "num_predict": 200})
    return chat_api(MODEL_IDS["LLaMA 3"], full_prompt, opt)

# ==============================
# LOGIN SIMPLES
# ==============================
if "usuario" not in st.session_state:
    st.session_state.usuario = "Convidado"
    st.session_state.plano = "Ultra"

st.sidebar.title(f"ğŸ¤– PrimeBud â€” {st.session_state.usuario}")
st.sidebar.markdown(f"**Plano:** {st.session_state.plano}")
st.sidebar.link_button("ğŸŒ Ver no GitHub", GITHUB_URL)
st.sidebar.divider()

modo = st.sidebar.selectbox("Modo:", list(MODOS_DESC.keys()), index=1)
st.sidebar.info(MODOS_DESC[modo])

# ==============================
# ÃREA DO CHAT
# ==============================
st.title("ğŸš€ PrimeBud Turbo 1.1 â€” Groq Edition")

if "chat" not in st.session_state:
    st.session_state.chat = []

for m in st.session_state.chat:
    estilo = "background:#2b313e;color:#fff" if m["autor"] == "user" else "background:#ececf1;color:#000"
    st.markdown(f"<div style='{estilo};padding:10px;border-radius:10px;margin:6px 0;'><b>{m['autor']}:</b> {m['texto']}</div>", unsafe_allow_html=True)

msg = st.chat_input("Digite sua mensagem...")
if msg:
    st.session_state.chat.append({"autor": "VocÃª", "texto": msg})
    with st.spinner("Processando..."):
        resposta = gerar_resposta(modo, msg)
    st.session_state.chat.append({"autor": "PrimeBud", "texto": resposta})
    st.rerun()

